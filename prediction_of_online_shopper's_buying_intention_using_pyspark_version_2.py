# -*- coding: utf-8 -*-
"""Prediction of online shopper's  buying  intention using PySpark Version 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xex2k-JX99HXB2ihWUOwl0TnYpgYiztp
"""

!java --version



!python --version

# Install pyspark
!pip install pyspark
# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder\
        .config("/root/.ivy2/jars", "mjuez_approx-smote-1.1.2.jar") \
        .master("local[*]")\
        .appName('Projectversion2') \
        .getOrCreate()
# Check Spark Session Information
spark

import pyspark
print(pyspark.__version__)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

pip install handyspark

from pyspark.sql.functions import min,max,sum,count,current_date,current_timestamp
from pyspark.sql.functions import *
from pyspark.sql.functions import col,isnan,when,count
from pyspark.ml.feature import PCA
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler, StandardScaler,MinMaxScaler
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from sklearn.metrics import classification_report, confusion_matrix
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import Normalizer
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from handyspark import *
from pyspark.mllib.evaluation import BinaryClassificationMetrics
from pyspark.ml.classification import LinearSVC
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.ml.linalg import SparseVector, DenseVector

"""#**Loading The  Data**"""

df =spark.read.format("csv").option("header", "true").option("inferSchema", "true").option("delimiter", ",").load("online_shoppers_intention.csv")

df.show(5)

df.printSchema()

"""**Our dataset has 10 numerical features and 8 categorical features.**

***Numerical ***


Administrative: The number of pages of administrative type that the user visited for  example  login page.

Administrative_Duration: The amount of time spent on the adminitrative pages.

Informational: The number of pages of informational type that the user visited for  example  contact us page.

Informational_Duration: The amount of time spent on the informational pages.

ProductRelated: The number of pages of product pages type that the user visited for  example  those webpage  which  sells  products.

ProductRelated_Duration: The amount of time spent on the product pages.

BounceRates: The percentage of visitors who enter the website on that page and exit without triggering any additional tasks.

ExitRates: The percentage of pageviews on the website that end at that specific page.

PageValues: The average value of the page averaged over the value of the target page and/or the completion of an eCommerce transaction.

SpecialDay: This value represents the closeness of the browsing date to special days or holidays (eg Mother's Day or Valentine's day) in which the transaction is more likely to be finalized.


***Categorical***

Month: Contains the month the pageview occurred, in string form.

OperatingSystems: An integer value representing the operating system that the user was on when viewing the page.

Browser: An integer value representing the browser that the user was using to view the page.

Region: An integer value representing which region the user is located in.

TrafficType: An integer value representing what type of traffic the user is categorized into.
Read more about traffic types here.

VisitorType: A string representing whether a visitor is New Visitor, Returning Visitor, or Other.

Weekend: A boolean representing whether the session is on a weekend.

Revenue: A boolean representing whether or not the user completed the purchase.
"""

df.select("Weekend").distinct().show()

df.select("Revenue").distinct().show()

df.select('OperatingSystems').distinct().show()

df.select('Region').distinct().show()

df.select('SpecialDay').distinct().show()

df.select("TrafficType").distinct().show()

df.select("BounceRates").distinct().show()

df.select("ExitRates").distinct().show()

df.agg({"Administrative_Duration": 'min', "Informational_Duration": 'min', "ProductRelated_Duration":'min' }).show()

df.agg({"Administrative_Duration": 'max', "Informational_Duration": 'max', "ProductRelated_Duration":'max' }).show()

df.agg({"Administrative": 'min', "Informational": 'min' ,"ProductRelated":'min'}).show()

df.agg({"Administrative": 'max', "Informational": 'max',"ProductRelated":'max' }).show()

df.groupBy('Revenue').agg(count('Revenue')).show()

df.groupBy('VisitorType').agg(count('VisitorType')).show()

df.groupBy('Region').agg(count('Region')).show()

df.groupBy('TrafficType').agg(count('TrafficType')).show()

df.groupBy('SpecialDay').agg(count('SpecialDay')).show()

df.groupBy('Browser').agg(count('Browser')).show()

df.groupBy('OperatingSystems').agg(count('OperatingSystems')).show()

df.groupBy('Weekend').agg(count('Weekend')).show()

#since  they  are  boolean  types  and thus have  two  values  we  can  easily  convert  them  into  integer

df=df.withColumn("Weekend",df.Weekend.cast('integer'))
df=df.withColumn("Revenue",df.Revenue.cast('integer'))

# Since  These  are  categorical  columns and  are in numerical  form  so first  I  willconvert them into  String
# and  then  apply  one hot encoding  later

df=df.withColumn("Month",df.Month.cast('String'))
df=df.withColumn("OperatingSystems",df.OperatingSystems.cast('String'))
df=df.withColumn("Browser",df.Browser.cast('String'))
df=df.withColumn("Region",df.Region.cast('String'))
df=df.withColumn("TrafficType",df.TrafficType.cast('String'))
df=df.withColumn("VisitorType",df.VisitorType.cast('String'))

df.printSchema()

#Checking Again

df.groupBy('Revenue').agg(count('Revenue')).show()

#True  is converted in to  class 1
#False into  Class 0

"""**The func for  cheching  null  values  works  only  on numeric  or  string  thats why  we  converted  weekend  and  revenue  to  int and  also  we  need  to  convert  them into  int  because  model  works  only on  numeric  values.
Since  they  are  boolean  we  can  simply  cast  them  as integer  to  convert  into  integer.**
"""

df2 = df.select([count(when(col(c).contains('None') | \
                            col(c).contains('NULL') | \
                            (col(c) == '' ) | \
                            col(c).isNull() | \
                            isnan(c), c
                           )).alias(c)
                    for c in df.columns])
df2.show()

"""
Plotting  numeric columns to find any correlation"""

numeric_features = [t[0] for t in df.dtypes if t[1] == 'integer' or  t[1] == 'double' or t[1] == 'int']
numeric_data = df.select(numeric_features).toPandas()
axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));
n = len(numeric_data.columns)

for i in range(n):
    v = axs[i, 0]
    v.yaxis.label.set_rotation(0)
    v.yaxis.label.set_ha('right')
    v.set_yticks(())
    h = axs[n-1, i]
    h.xaxis.label.set_rotation(90)
    h.set_xticks(())


    #  Exitrates and bouncerates  are highly correlated.

#Plotting Scatter matrix  using  Seaborn
#numeric_features = [t[0] for t in df.dtypes if t[1] == 'integer' or  t[1] == 'double' or t[1] == 'int']
#numeric_data = df.select(numeric_features).toPandas()



#sns.set_theme(style="ticks")
#sns.pairplot(numeric_data)

"""Visualization

**To perform the visualization of data, it is necessary to convert all features to numerical form.  We  will convert  them by using the Sklearn library.**

**For visualization   we need  to  convert  pyspark  dataframe  into  pandas  dataframe  because  we are  using  pandas  matplotlib and  seaborn  library  to  visualize  data.**
"""

dfpandas=df.toPandas()

dfpandas.info()

dfpandas.describe().transpose()

#Administrative, Informational, Product related page and their corresponding duration can be zero (e.g. don't visit and spend time on that page) but cannot be a negative value visit
#Similarly you cannot have negative values for Bounce Rates, Exit rates, Page values, Operating system, Browser, Region

#Data  is  imbalanced
plt.figure(figsize=(10,6))
sns.countplot(x='Revenue',data=dfpandas, palette = 'pastel')
plt.title('Revenue', fontsize = 30)
plt.xlabel('Revenue or not', fontsize = 15)
plt.ylabel('count', fontsize = 15)

#Page Metrics Analysis

fig = plt.figure(figsize=(16, 4))

ax1 = fig.add_subplot(1, 3, 1)
ax2 = fig.add_subplot(1, 3, 2)
ax3 = fig.add_subplot(1, 3, 3)

sns.distplot(dfpandas['BounceRates'], bins=20, ax=ax1)
sns.distplot(dfpandas['ExitRates'], bins=20, ax=ax2)
sns.distplot(dfpandas['PageValues'], bins=20, ax=ax3)

plt.tight_layout()
plt.show()

#  All  these  three are not  normalized
# And all  these  three  are slightly right  skewed
# All  three distributions have  lot of outliers.
# Most values concentrated near 0

#Page Metrics Analysis

fig = plt.figure(figsize=(16, 4))

ax1 = fig.add_subplot(1, 3, 1)
ax2 = fig.add_subplot(1, 3, 2)
ax3 = fig.add_subplot(1, 3, 3)

sns.distplot(dfpandas['Administrative'], bins=20, ax=ax1)
sns.distplot(dfpandas['Informational'], bins=20, ax=ax2)
sns.distplot(dfpandas['ProductRelated'], bins=20, ax=ax3)

plt.tight_layout()
plt.show()

#  All  these  three are not  normalized
# And all  these  three  are slightly right  skewed

#Page Metrics Analysis

fig = plt.figure(figsize=(16, 4))

ax1 = fig.add_subplot(1, 3, 1)
ax2 = fig.add_subplot(1, 3, 2)
ax3 = fig.add_subplot(1, 3, 3)

sns.distplot(dfpandas['Administrative_Duration'], bins=10, ax=ax1)
sns.distplot(dfpandas['Informational_Duration'], bins=10, ax=ax2)
sns.distplot(dfpandas['ProductRelated_Duration'], bins=10, ax=ax3)

plt.tight_layout()
plt.show()

#  All  these  three are not  normalized
# And all  these  three  are slightly right  skewed
# Values  areconcentrated near  one  of  the  bin

fig = plt.figure(figsize=(12, 12))

ax1 = fig.add_subplot(2, 3, 1)
ax2 = fig.add_subplot(2, 3, 2)
ax3 = fig.add_subplot(2, 3, 3)
ax4 = fig.add_subplot(2, 3, 4)
ax5 = fig.add_subplot(2, 3, 5)
ax6 = fig.add_subplot(2, 3, 6)

sns.boxplot(data=dfpandas, x = 'Revenue', y = 'Administrative', ax=ax1)
sns.boxplot(data=dfpandas, x = 'Revenue', y = 'Informational', ax=ax2)
sns.boxplot(data=dfpandas, x = 'Revenue', y = 'ProductRelated', ax=ax3)
sns.boxplot(data=dfpandas, x = 'Revenue', y = 'Administrative_Duration', ax=ax4)
sns.boxplot(data=dfpandas, x = 'Revenue', y = 'Informational_Duration', ax=ax5)
sns.boxplot(data=dfpandas, x = 'Revenue', y = 'ProductRelated_Duration', ax=ax6)

plt.tight_layout()
plt.show()


#From the above boxplots, we can see that:

#In general, visitors tend to visit less pages, and spend less time, if they are not going to make a purchase.
#The number of product related pages, and the time spent on them, is way higher than that for account related or informational pages.
#The first 3 feature look like they follow a skewed normal distribution.
#For effective revenue genration low bounce rates, low exit rates and high page values hold the key.



#No outlier treatment will be made. We assume outliers are significant for the machine learning model
# If we remove  outlier  many  data  will be  deleted  ,  thus  leaving very  less rows.

#Visitor Analysis

fig = plt.figure(figsize=(18, 6))

ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
ax4 = fig.add_subplot(2, 2, 4)

sns.countplot(data=dfpandas, x='OperatingSystems', hue='VisitorType', ax=ax1)
sns.countplot(data=dfpandas, x='Browser', hue='VisitorType', ax=ax2)
sns.countplot(data=dfpandas, x='Region', hue='VisitorType', ax=ax3)
sns.countplot(data=dfpandas, x='TrafficType', hue='VisitorType', ax=ax4)

ax1.legend(loc='upper right')
ax2.legend(loc='upper right')
ax3.legend(loc='upper right')
ax4.legend(loc='upper right')
plt.tight_layout()
plt.show()


# Only one Operating system is responsible for maximum number of entries  in our dataset
# Operating System number  from 4 of the 8 operating systems were not used much  while  browsing the  website .
#  Same  case  with browsers used by visitors, where there is 1 dominant browser, 5 with decent representation in the dataset, and the rest are rarey used.
#  Traffic  comes from  various  sources and Also Traffic sources are very diverse, with a few that did not contribute much to the dataset.
# Few  regions have  more  users  compare  to  others

#Visitor Analysis

fig = plt.figure(figsize=(18, 6))

ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
ax4 = fig.add_subplot(2, 2, 4)

sns.countplot(data=dfpandas, x='OperatingSystems', hue='Revenue', ax=ax1)
sns.countplot(data=dfpandas, x='Browser', hue='Revenue', ax=ax2)
sns.countplot(data=dfpandas, x='Region', hue='Revenue', ax=ax3)
sns.countplot(data=dfpandas, x='TrafficType', hue='Revenue', ax=ax4)

ax1.legend(loc='upper right')
ax2.legend(loc='upper right')
ax3.legend(loc='upper right')
ax4.legend(loc='upper right')
plt.tight_layout()
plt.show()

fig = plt.figure(figsize=(18, 12))

ax1 = fig.add_subplot(2, 1, 1)
ax2 = fig.add_subplot(2, 1, 2)


sns.countplot(data=dfpandas, x='Month', hue='Revenue', ax=ax1)
sns.countplot(data=dfpandas, x='SpecialDay', hue='Revenue', ax=ax2)


plt.tight_layout()
plt.show()



#On March and May, we have a lot of visits (May is the month with the highest number of visits), yet transactions made during those 2 months are not on the same level.
#We have no visits at all during Jan nor Apr.
#Most transactions happen during the end of the year, with Nov as the month with the highest number of confirmed transactions.
#The closer the visit date to a special day (like black Friday, new year's, ... etc) the more likely it will end up in a transaction.
#Most of transactions happen on special days (SpecialDay =0).

fig = plt.figure(figsize=(12, 12))

ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)

colors = ['yellow', 'blue']
ax1.pie(dfpandas['Weekend'].value_counts(),explode=(0.1,0.1),labels=['Weekday','Weekend'], colors=colors,autopct='%1.0f%%')
ax1.set_title('Weekend vs. Weekday (Total Visits)')
ax2.pie(dfpandas[dfpandas['Revenue'] == 1]['Weekend'].value_counts(),explode=(0.1,0.1),labels=['Weekday','Weekend'], colors=colors,autopct='%1.0f%%')
ax2.set_title('Weekend vs. Weekday (Visits which  have Revenue  column  value  as  1)')
plt.show()

#most  transactions  were  on  weekdays  itself.

#Heatmap   taking  all  columns  not  just  numeric


plt.figure(figsize=(16,12))
sns.heatmap(data=dfpandas.corr(),annot=True,cmap='RdYlGn')

#From the above heatmap, we observe the following:

#In general, there is very little correlation among the different features in our dataset.
#The very few cases of high correlation (|corr| >= 0.7) are:
#BounceRates & ExitRates (0.91).
#ProductRelated & ProductRelated_Duration (0.86).

plt.figure(figsize=(10,6))
sns.scatterplot(x="BounceRates", y="ExitRates",hue = 'Revenue', data=dfpandas)

sns.scatterplot(x="ProductRelated", y="ProductRelated_Duration",hue = 'Revenue', data=dfpandas)


#Productand ProuctRelated_Duration arealso  corelated  but  not  much

df=df.drop("BounceRates")

"""**Data preprocessing**"""

month_indexer = StringIndexer(inputCol="Month", outputCol="monthIndex")
visitortype_indexer = StringIndexer(inputCol="VisitorType", outputCol="visitortypeIndex")

#Since  they  are  categorical  columns  and  I  want to apply one hot encoding on it that is why I am  passing thme in StingIndexer  so  that  they  gets  converted  into vector  form

ope_indexer= StringIndexer(inputCol="OperatingSystems", outputCol="OperatingSystemsIndex")
reg_indexer= StringIndexer(inputCol="Region", outputCol="RegionIndex")
brow_indexer= StringIndexer(inputCol="Browser", outputCol="BrowserIndex")
tra_indexer= StringIndexer(inputCol="TrafficType", outputCol="TrafficTypeIndex")
week_indexer= StringIndexer(inputCol="Weekend", outputCol="WeekendIndex")

onehotencoder_month_vector = OneHotEncoder(inputCol="monthIndex", outputCol="month_vec")
onehotencoder_visitortype_vector = OneHotEncoder(inputCol="visitortypeIndex", outputCol="visitortype_vec")
onehotencoder_ope_vector= OneHotEncoder(inputCol="OperatingSystemsIndex", outputCol="OperatingSystems_vec")
onehotencoder_reg_vector= OneHotEncoder(inputCol="RegionIndex", outputCol="Region_vec")
onehotencoder_brow_vector= OneHotEncoder(inputCol="BrowserIndex", outputCol="Browser_vec")
onehotencoder_tra_vector= OneHotEncoder(inputCol="TrafficTypeIndex", outputCol="TrafficType_vec")
onehotencoder_week_vector= OneHotEncoder(inputCol="WeekendIndex", outputCol="Weekend_vec")

#vectorization method  can  workson boolean  but the  model  needs  numeric values  thats why I  converted  Weekend and Revenue from  boolean  to  integer

pipeline = Pipeline(stages=[month_indexer,
                            visitortype_indexer,
                            ope_indexer,
                            reg_indexer,
                            brow_indexer,
                            tra_indexer,
                            week_indexer,
                            onehotencoder_month_vector,
                            onehotencoder_visitortype_vector,
                            onehotencoder_ope_vector,
                            onehotencoder_reg_vector,
                            onehotencoder_brow_vector,
                            onehotencoder_tra_vector,
                            onehotencoder_week_vector



                    ])

df_transformed = pipeline.fit(df).transform(df)
df_transformed.show()

df_transformed.printSchema()

df_transformed2=df_transformed.drop(*('VisitorType','Month','OperatingSystems','Browser','Region','TrafficType','Weekend','visitortypeIndex','monthIndex','OperatingSystemsIndex','RegionIndex','BrowserIndex','TrafficTypeIndex','WeekendIndex'))

df_transformed2.show(5)

df_transformed2.printSchema()

inputcolumns=['Administrative',
 'Administrative_Duration',
  'Informational',
  'Informational_Duration',
  'ProductRelated',
  'ProductRelated_Duration',
  'ExitRates',
  'PageValues',
  'SpecialDay',
   'month_vec',
  'visitortype_vec',
  'OperatingSystems_vec',
  'Region_vec',
  'Browser_vec',
  'TrafficType_vec',
  'Weekend_vec']

assembler = VectorAssembler(inputCols = inputcolumns, outputCol='features')
output = assembler.transform(df_transformed2)

#Below  we  see  two  vectors  that  is  because of  the  difference  in the type  of  the  data
#Such  type  of vector is  called  Sparsevector.

final_data = output.select('features', 'Revenue')
final_data.show(truncate=False)


#  These  are sparse vectors

"""# Applying  various  models for first run

#Logistic Regression
"""

train, test = final_data.randomSplit([0.8, 0.2],40)

lr = LogisticRegression(labelCol="Revenue",maxIter=20)
lrn = lr.fit(train)

predictions = lrn.transform(test)
predictions.show()

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Naive Bayes"""

# create the trainer and set its parameters
nb = NaiveBayes(modelType="multinomial",labelCol="Revenue")

model = nb.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""# Support Vector  Machine"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features",labelCol="Revenue",maxIter=50)


svmModel = svc.fit(train)

predictions = svmModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

"""#Decision  Tree"""

dt = DecisionTreeClassifier(labelCol="Revenue", featuresCol="features")

model = dt.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""# Random Forest"""

rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Revenue')
rfModel = rf.fit(train)
predictions = rfModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)

print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Conclusion of first  run

Since  our dataset is  small  so decision trees is  giving  higher  accuracy  than  random forest . In  above  all  models  Decision Tree  gave the highest  frequency without applying standard scaling  or applying normalizer.

#SECOND RUN

# Applying  Standard Scaling
"""

final_data.show(5,truncate=False)

train, test = final_data.randomSplit([0.8, 0.2],40)

standard_scaler = StandardScaler(inputCol="features", outputCol="features_scaled",withStd=True, withMean=False)

model = standard_scaler.fit(train)
train = model.transform(train)
test=model.transform(test)

train.show(5,truncate=False)

test.show(5,truncate=False)

train = train.select('features_scaled', 'Revenue')
test = test.select('features_scaled', 'Revenue')

train=train.withColumnRenamed("features_scaled","features")

test=test.withColumnRenamed("features_scaled","features")

train.show(5,truncate=False)

test.show(5,truncate=False)

"""#Applying various models for Second Run

# Logistic Regression
"""

lr = LogisticRegression(labelCol="Revenue",maxIter=10)
lrn = lr.fit(train)

predictions = lrn.transform(test)
predictions.show()

#Evaluating using Accuracy .

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""# Naive Bayes"""

# create the trainer and set its parameters
nb = NaiveBayes(modelType="multinomial",labelCol="Revenue")

model = nb.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#  Support Vector  Machine"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features",labelCol="Revenue",maxIter=50)

svmModel = svc.fit(train)

predictions = svmModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

"""#Decision  Tree"""

dt = DecisionTreeClassifier(labelCol="Revenue", featuresCol="features")

model = dt.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Random Forest"""

rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Revenue')
rfModel = rf.fit(train)
predictions = rfModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Conclusion of Second run

LogisticRegression Accuracy almost same \
NaiveBayes accuracy   Slightly reduced\
Decision tree got   the highest accuracy but remained same\
Random Forest Accuracy also  got changed

#Third Run
"""

final_data.show(5)

train, test = final_data.randomSplit([0.8, 0.2],40)

"""#  Applying MinMax  Scaling"""

minmax_scaler = MinMaxScaler(inputCol="features", outputCol="features_scaled")

model = minmax_scaler.fit(train)
train = model.transform(train)
test=model.transform(test)

train.show(5,truncate=False)

test.show(5,truncate=False)

train = train.select('features_scaled', 'Revenue')
test = test.select('features_scaled', 'Revenue')

train=train.withColumnRenamed("features_scaled","features")

test=test.withColumnRenamed("features_scaled","features")

train.show(5,truncate=False)

test.show(5,truncate=False)

"""# Applying Various  models for Third Run

#Logistic Regression
"""

lr = LogisticRegression(labelCol="Revenue", maxIter=10)
lrn = lr.fit(train)

predictions = lrn.transform(test)
predictions.show()

#Evaluating using accuracy

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#  Naive  Bayes"""

# create the trainer and set its parameters
nb = NaiveBayes(modelType="multinomial",labelCol="Revenue")

model = nb.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Support Vector Machine"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features",labelCol="Revenue",maxIter=50)

svmModel = svc.fit(train)

predictions = svmModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

"""#Decision Tree"""

dt = DecisionTreeClassifier(labelCol="Revenue", featuresCol="features")

model = dt.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""# Random Forest"""

rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Revenue')
rfModel = rf.fit(train)
predictions = rfModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Conclusion of third run

Results are similar  to  previous run  except  NaiveBayes.  Naive  Bayes accuracy  got decreased .

# Applying  PCA

In  this  portion we are  performing data  Preprocessing  before  PCA,  Since  our data  has corelation  that  is  why  we  are  applying  PCA ,  because  PCA converges  all  the  correlated  variables and  reduces dimensionality.

For  this  portion I  am  taking  Original  Dataframe  which  was before dropping 'BounceRates'.  Since'Bouncerates' has strong  corelaton  with 'ExitRates'.

As given in the above code  ,  I  dropped 'BounceRates'  from  df and  stored the same in  df  itself.  That  is  why  I  am not  using  df  .

I  am  creating  the dataframe  which  contained  all  columns  from  the  dfpandas datframe  because it  has all  the  columns.
"""

df_pca = spark.createDataFrame(dfpandas)

df_pca.printSchema

month_indexer = StringIndexer(inputCol="Month", outputCol="monthIndex")
visitortype_indexer = StringIndexer(inputCol="VisitorType", outputCol="visitortypeIndex")

#Since  they  are  categorical  columns  and  I  want to apply one hot encoding on it that is why I am  passing thme in StingIndexer  so  that  they  gets  converted  into vector  form

ope_indexer= StringIndexer(inputCol="OperatingSystems", outputCol="OperatingSystemsIndex")
reg_indexer= StringIndexer(inputCol="Region", outputCol="RegionIndex")
brow_indexer= StringIndexer(inputCol="Browser", outputCol="BrowserIndex")
tra_indexer= StringIndexer(inputCol="TrafficType", outputCol="TrafficTypeIndex")
week_indexer= StringIndexer(inputCol="Weekend", outputCol="WeekendIndex")

onehotencoder_month_vector = OneHotEncoder(inputCol="monthIndex", outputCol="month_vec")
onehotencoder_visitortype_vector = OneHotEncoder(inputCol="visitortypeIndex", outputCol="visitortype_vec")
onehotencoder_ope_vector= OneHotEncoder(inputCol="OperatingSystemsIndex", outputCol="OperatingSystems_vec")
onehotencoder_reg_vector= OneHotEncoder(inputCol="RegionIndex", outputCol="Region_vec")
onehotencoder_brow_vector= OneHotEncoder(inputCol="BrowserIndex", outputCol="Browser_vec")
onehotencoder_tra_vector= OneHotEncoder(inputCol="TrafficTypeIndex", outputCol="TrafficType_vec")
onehotencoder_week_vector= OneHotEncoder(inputCol="WeekendIndex", outputCol="Weekend_vec")

#vectorization method  can  workson boolean  but the  model  needs  numeric values  thats why I  converted  Weekend and Revenue from  boolean  to  integer

pipeline = Pipeline(stages=[month_indexer,
                            visitortype_indexer,
                            ope_indexer,
                            reg_indexer,
                            brow_indexer,
                            tra_indexer,
                            week_indexer,
                            onehotencoder_month_vector,
                            onehotencoder_visitortype_vector,
                            onehotencoder_ope_vector,
                            onehotencoder_reg_vector,
                            onehotencoder_brow_vector,
                            onehotencoder_tra_vector,
                            onehotencoder_week_vector



                    ])

df_transformed_pca = pipeline.fit(df_pca).transform(df_pca)
df_transformed_pca.show()

df_transformed_pca.printSchema()

df_transformed_pca=df_transformed_pca.drop(*('VisitorType','Month','OperatingSystems','Browser','Region','TrafficType','Weekend','visitortypeIndex','monthIndex','OperatingSystemsIndex','RegionIndex','BrowserIndex','TrafficTypeIndex','WeekendIndex'))

df_transformed_pca.show(5)

df_transformed_pca.printSchema()

inputcolumns=['Administrative',
 'Administrative_Duration',
  'Informational',
  'Informational_Duration',
  'ProductRelated',
  'ProductRelated_Duration',
  'BounceRates',
  'ExitRates',
  'PageValues',
  'SpecialDay',
   'month_vec',
  'visitortype_vec',
  'OperatingSystems_vec',
  'Region_vec',
  'Browser_vec',
  'TrafficType_vec',
  'Weekend_vec']

assembler = VectorAssembler(inputCols = inputcolumns, outputCol='features')
output = assembler.transform(df_transformed_pca)

final_data_pca = output.select('features', 'Revenue')
final_data_pca.show(truncate=False)

"""#Standardizing  the  data before  using  PCA using Normalizer."""

normalizer = Normalizer(inputCol="features", outputCol="features_scaled", p=1.0)

train, test = final_data_pca.randomSplit([0.8, 0.2],40)

final_pipeline = Pipeline(
    stages = [normalizer]
)

pipelineModel = final_pipeline.fit(train)
train = pipelineModel.transform(train)
test = pipelineModel.transform(test)

train = train.select('features_scaled', 'Revenue')
test = test.select('features_scaled', 'Revenue')

train=train.withColumnRenamed("features_scaled","features")

test=test.withColumnRenamed("features_scaled","features")

#Initially I  am  taking K  as 17  because I  want  to  find varaince  and then  based  on  the values of explained varaince ,
# the  componenets  which  explains  around  90%  of  the  variation , I  will  take  that  number as K.

pca = PCA(k=17, inputCol="features", outputCol="pcafeatures") # here I Have defined maximum number of features that I have
model = pca.fit(train) # fit the data to pca to make the model
train = model.transform(train)
test = model.transform(test)



train = train.select('pcafeatures', 'Revenue')
test = test.select('pcafeatures', 'Revenue')

train=train.withColumnRenamed("pcafeatures","features")

test=test.withColumnRenamed("pcafeatures","features")



# Scree Plot
_ = sns.set(style='whitegrid', font_scale=1.2)
fig, ax = plt.subplots(figsize=(18, 10))

PC_components=np.arange(model.getK()) + 1

_ = sns.barplot(x=PC_components, y=model.explainedVariance, color='b')
_ = sns.lineplot(x=PC_components-1, y=model.explainedVariance.cumsum(), color='black', linestyle='-', linewidth=2, marker='o', markersize=8)

plt.title('Scree Plot')
plt.xlabel('N-th Principal Component')
plt.ylabel('Variance Explained')
plt.ylim(0, 1)
plt.show()

"""# As  shown in  the above graph the  ideal  number  of components  for PCA came  out to  be  6."""

pca = PCA(k=6, inputCol='features', outputCol='pcaFeature')

"""# Applying  various  models for fourth run

#Logistic Regression
"""

lr = LogisticRegression (featuresCol="pcaFeature",labelCol="Revenue",maxIter=10)
pipeline = Pipeline (stages=[pca, lr])
model = pipeline.fit(train)
#train=model.transform(train)  #  No  Need  and also  if  we  do  this  then  train will have  PCSFeaturecolumn an  d  other jobs wil not run
predictions =model.transform(test)

predictions.show()

#Evaluating using Accuracy .

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Support Vector  Machine"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="pcaFeature",labelCol="Revenue",maxIter=50)
pipelinesvc = Pipeline (stages=[pca,svc])
svmModel = pipelinesvc.fit(train)
#train=svmModel.transform(train)
 #No needto  transform  train, main thing is  weshould fittrain and  transform  test,  wetransform  train when  wewant to findscoreoftraining data

predictions = svmModel.transform(test)
predictions.show()


#linearSvc in Pyspark  for binary  classification  problems

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

"""#  Decision  tree"""

dt = DecisionTreeClassifier(featuresCol="pcaFeature",labelCol="Revenue")
pipeline = Pipeline (stages=[pca, dt])
model = pipeline.fit(train)

predictions =model.transform(test)

predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Random Forest"""

rf = RandomForestClassifier(featuresCol = 'pcaFeature', labelCol = 'Revenue')
pipeline = Pipeline (stages=[pca, rf])
model = pipeline.fit(train)

predictions =model.transform(test)

predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Conclusion of Fourth Run

All of the models accuracy  got decreased .  Naive  Bayes  was  not  working  because after scaling  one of  the  vector   value was  negative  and  naive bayes  cannot work  with negative values  in vector .

Random Forestaccuracy got  increased  a lot.
Rest algorithms worked but showed decrease  in  accuracy.

#Fifth Run
"""

final_data.show(5, truncate=False)

"""#  Applying  Normalization on the  data"""

normalizer = Normalizer(inputCol="features", outputCol="features_norm", p=1.0)
train, test = final_data.randomSplit([0.8, 0.2],40)

final_pipeline = Pipeline(
    stages = [normalizer]
)

pipelineModel = final_pipeline.fit(train)
train = pipelineModel.transform(train)
test = pipelineModel.transform(test)

train = train.select('features_norm', 'Revenue')
test = test.select('features_norm', 'Revenue')

train.show(5,truncate=False)

test.show(5,truncate=False)

"""#Logistic Regression"""

lr = LogisticRegression(featuresCol='features_norm',labelCol="Revenue",maxIter=10)
lrn = lr.fit(train)

predictions = lrn.transform(test)
predictions.show()

#Evaluating using Accuracy .

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""# Naive  Bayes"""

# create the trainer and set its parameters
nb = NaiveBayes(featuresCol='features_norm',modelType="multinomial",labelCol="Revenue")

model = nb.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Support Vector Machine"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features_norm",labelCol="Revenue",maxIter=50)

svmModel = svc.fit(train)

predictions = svmModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

"""#  Decision  Tree"""

dt = DecisionTreeClassifier(featuresCol='features_norm',labelCol="Revenue")

model = dt.fit(train)

predictions = model.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Random  Forest"""

rf = RandomForestClassifier(featuresCol='features_norm', labelCol = 'Revenue')
rfModel = rf.fit(train)
predictions = rfModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Gradient Boosting"""

gbt = GBTClassifier(featuresCol="features_norm", labelCol = 'Revenue',maxIter=10)
gbtModel = gbt.fit(train)
predictions = gbtModel.transform(test)
predictions.show()

# compute accuracy on the test set
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
acu = evaluator.evaluate(predictions)
print("Test set accuracy  = " + str(acu))

y_true = predictions.select("Revenue")
y_true = y_true.toPandas()

y_pred = predictions.select("prediction")
y_pred = y_pred.toPandas()

cnf_matrix = confusion_matrix(y_true, y_pred,labels=[0,1])
print(cnf_matrix)


print(classification_report(y_true, y_pred, target_names=["0","1"]))

bcm = BinaryClassificationMetrics(predictions, scoreCol='probability', labelCol='Revenue')
# Get metrics from evaluator
print("Area under ROC Curve: {:.4f}".format(bcm.areaUnderROC))
print("Area under PR Curve: {:.4f}".format(bcm.areaUnderPR))
# Plot both ROC and PR curves
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
bcm.plot_roc_curve(ax=axs[0])
bcm.plot_pr_curve(ax=axs[1])

"""#Conclusion of the fifth run

        
If we compare to previous stage then after applying  normalization little  bit accuracy got  improved but  preciosn and  recall  worsened  . And  this  stage  I  also  applied  gradient boosting which gave very good accuracy .

#Sixth Run

#Hyper Parameter Tuning with 5 folds.

The  Second  Run  of  applying standard Scaling gavethe  highest score  so I  decided  that  I will use  the standardscaled  data  for  hyper parameter tuning.
Earlier I  was  taking normlaized  data  but  that did not show improvemnet in accuracy compare to  previous  run .  So I  took standard scaled  data.
"""

timestamp = pd.Timestamp(datetime.datetime(2023, 5, 27))

final_data.show(5,truncate=False)

train, test = final_data.randomSplit([0.8, 0.2],40)

standard_scaler = StandardScaler(inputCol="features", outputCol="features_scaled",withStd=True, withMean=False)

model = standard_scaler.fit(train)
train = model.transform(train)
test=model.transform(test)

train = train.select('features_scaled', 'Revenue')
test = test.select('features_scaled', 'Revenue')

"""#  Hyper ParameterTuning  For  logistic regression"""

lr = LogisticRegression(featuresCol='features_scaled', labelCol='Revenue')

paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01,0.001,0.0001]) \
    .addGrid(lr.fitIntercept, [False, True])\
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\
    .addGrid(lr.maxIter, [5,10,20,30,40,50])\
    .build()

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


lrcv = CrossValidator(estimator=lr,
                          estimatorParamMaps=paramGrid,
                          evaluator=evaluator,
                          numFolds=5)



# getting the current date and time
start = timestamp.today()

# Run cross-validation, and choose the best set of parameters.
lrcvModel = lrcv.fit(train)
#print(lrcvModel)

end = timestamp.today()

training_time=end-start

print(training_time)

# Make predictions on testing data and calculating ROC metrics and model accuracy.
predictions = lrcvModel.transform(test)


print('Accuracy:', evaluator.evaluate(predictions))

bestModel = lrcvModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For Naive Bayes"""

#Create initial Naïve Bayes model
nb = NaiveBayes(featuresCol="features_scaled",labelCol="Revenue")

# Create ParamGrid for Cross Validation
nbparamGrid = (ParamGridBuilder()
               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
               .build())

# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
nbcv = CrossValidator(estimator = nb,
                    estimatorParamMaps = nbparamGrid,
                    evaluator = evaluator,
                    numFolds = 5)



# getting the current date and time
start = timestamp.today()


# Run cross validations
nbcvModel = nbcv.fit(train)
#print(nbcvModel)

end = timestamp.today()

training_time=end-start

print(training_time)


# Use test set here so we can measure the accuracy of our model on new data
nbpredictions = nbcvModel.transform(test)

# cvModel uses the best model found from the Cross Validation
# Evaluate best model
print('Accuracy:', evaluator.evaluate(nbpredictions))

bestModel = nbcvModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For SupportVector Machine

"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features_scaled",labelCol="Revenue")

svcparamGrid = (ParamGridBuilder()
            .addGrid(svc.maxIter, [5,10,20,30,40,50])
             .addGrid(svc.regParam, [1, 0.1, 0.01, 0.001, 0.0001])
              .build())



# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
svmcv = CrossValidator(estimator = svc,
                    estimatorParamMaps = svcparamGrid,
                    evaluator = evaluator,
                    numFolds = 5)



# getting the current date and time
start = timestamp.today()

# Run cross validations
svmModel = svmcv.fit(train)
#print(svmModel)

end = timestamp.today()

training_time=end-start

print(training_time)


# Use test set here so we can measure the accuracy of our model on new data
svmpredictions = svmModel.transform(test)


# Evaluate best model
print('Accuracy:', evaluator.evaluate(svmpredictions))

bestModel = svmModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For Decision  Tree"""

dt = DecisionTreeClassifier(featuresCol='features_scaled',labelCol="Revenue")

dtparamGrid = ParamGridBuilder() \
    .addGrid(dt.maxDepth, [3, 5, 7]) \
    .addGrid(dt.minInstancesPerNode, [1, 3, 5]) \
    .build()



# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
dtcv = CrossValidator(estimator = dt,
                    estimatorParamMaps = dtparamGrid,
                    evaluator = evaluator,
                    numFolds = 5)


# getting the current date and time
start = timestamp.today()

# Run cross validations
dtModel = dtcv.fit(train)
#print(dtModel)

end = timestamp.today()

training_time=end-start

print(training_time)

# Use test set here so we can measure the accuracy of our model on new data
dtpredictions = dtModel.transform(test)


# Evaluate best model
print('Accuracy:', evaluator.evaluate(dtpredictions))
bestModel = dtModel.bestModel

print (bestModel)

"""#Gradient boosting with hyperparameter tuning"""

gbt = GBTClassifier(featuresCol="features_scaled", labelCol = 'Revenue',maxIter=10)

gbparamGrid = (ParamGridBuilder()
            .addGrid(gbt.maxDepth, [2, 5, 10])
             .addGrid(gbt.maxBins, [10, 20, 40])
             .addGrid(gbt.maxIter, [5, 10, 20])
             .build())
gbevaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
# Create 5-fold CrossValidator
gbcv = CrossValidator(estimator = gbt,
                      estimatorParamMaps = gbparamGrid,
                      evaluator = gbevaluator ,
                      numFolds = 5)



# getting the current date and time
start = timestamp.today()

gbcvModel = gbcv.fit(train)

end = timestamp.today()

training_time=end-start

print(training_time)

gbpredictions = gbcvModel.transform(test)
print('Accuracy:', gbevaluator.evaluate(gbpredictions))
bestModel = gbcvModel.bestModel

# Best  Hyperparameters
print('numTrees - ', bestModel.getNumTrees)
print('maxDepth - ', bestModel.getOrDefault('maxDepth'))

"""#Hyper parameter tuning for Random forest"""

rf = RandomForestClassifier(featuresCol='features_scaled', labelCol = 'Revenue')
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \
    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \
    .build()
rfevaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
rfhypertuning = CrossValidator(estimator=rf,
                          estimatorParamMaps=paramGrid,
                          evaluator=rfevaluator,
                          numFolds=5)



# getting the current date and time
start = timestamp.today()

rfhypertuning_model = rfhypertuning.fit(train)

end = timestamp.today()

training_time=end-start

print(training_time)


rfpredictions = rfhypertuning_model.transform(test)
print('Accuracy:', rfevaluator.evaluate(rfpredictions))
bestModel = rfhypertuning_model.bestModel

# Best  Hyperparameters
print('numTrees - ', bestModel.getNumTrees)
print('maxDepth - ', bestModel.getOrDefault('maxDepth'))

"""In  the  previous  stage Gradient  boosting gave  similar accuracy to Decesion  tree as  its  base model  is  Decision  Tree.  We can  use any  model as  reference inside Gradient Boosting . I  have kept default  value  only.  I  did not change  it  so  gradient boosting  took decision tree  as  its  base  model.  Afte rapplying hyper  parametertuning  gradient  boosting  accuracy  showed  good  improvement .  Similarly  other models  showed  improvement.

#Seventh  Run

# Hyper Parameter Tuning with 10 folds.

#  Hyper ParameterTuning  For  logistic regression
"""

lr = LogisticRegression(featuresCol='features_scaled', labelCol='Revenue')

paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01,0.001,0.0001]) \
    .addGrid(lr.fitIntercept, [False, True])\
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\
    .addGrid(lr.maxIter, [5,10,20,30,40,50])\
    .build()

evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


lrcv = CrossValidator(estimator=lr,
                          estimatorParamMaps=paramGrid,
                          evaluator=evaluator,
                          numFolds=10)



# getting the current date and time
start = timestamp.today()

# Run cross-validation, and choose the best set of parameters.
lrcvModel = lrcv.fit(train)
#print(lrcvModel)

end = timestamp.today()

training_time=end-start

print(training_time)

# Make predictions on testing data and calculating ROC metrics and model accuracy.
predictions = lrcvModel.transform(test)


print('Accuracy:', evaluator.evaluate(predictions))


bestModel = lrcvModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For Naive Bayes"""

#Create initial Naïve Bayes model
nb = NaiveBayes(featuresCol="features_scaled",labelCol="Revenue")

# Create ParamGrid for Cross Validation
nbparamGrid = (ParamGridBuilder()
               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
               .build())

# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
nbcv = CrossValidator(estimator = nb,
                    estimatorParamMaps = nbparamGrid,
                    evaluator = evaluator,
                    numFolds = 10)



# getting the current date and time
start = timestamp.today()


# Run cross validations
nbcvModel = nbcv.fit(train)
#print(nbcvModel)

end = timestamp.today()

training_time=end-start

print(training_time)


# Use test set here so we can measure the accuracy of our model on new data
nbpredictions = nbcvModel.transform(test)

# cvModel uses the best model found from the Cross Validation
# Evaluate best model
print('Accuracy:', evaluator.evaluate(nbpredictions))

bestModel = nbcvModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For SupportVector Machine

"""

# instantiate classifier with default hyperparameters
svc=LinearSVC(featuresCol="features_scaled",labelCol="Revenue")

svcparamGrid = (ParamGridBuilder()
            .addGrid(svc.maxIter, [5,10,20,30,40,50])
             .addGrid(svc.regParam, [1, 0.1, 0.01, 0.001, 0.0001])
              .build())



# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
svmcv = CrossValidator(estimator = svc,
                    estimatorParamMaps = svcparamGrid,
                    evaluator = evaluator,
                    numFolds =10)



# getting the current date and time
start = timestamp.today()

# Run cross validations
svmModel = svmcv.fit(train)
#print(svmModel)

end = timestamp.today()

training_time=end-start

print(training_time)


# Use test set here so we can measure the accuracy of our model on new data
svmpredictions = svmModel.transform(test)


# Evaluate best model
print('Accuracy:', evaluator.evaluate(svmpredictions))


bestModel = svmModel.bestModel

print (bestModel)

"""#Hyper ParameterTuning For Decision  Tree"""

dt = DecisionTreeClassifier(featuresCol='features_scaled',labelCol="Revenue")

dtparamGrid = ParamGridBuilder() \
    .addGrid(dt.maxDepth, [3, 5, 7]) \
    .addGrid(dt.minInstancesPerNode, [1, 3, 5]) \
    .build()



# Evaluate model
evaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")


# Create 5-fold CrossValidator
dtcv = CrossValidator(estimator = dt,
                    estimatorParamMaps = dtparamGrid,
                    evaluator = evaluator,
                    numFolds = 10)


# getting the current date and time
start = timestamp.today()

# Run cross validations
dtModel = dtcv.fit(train)
#print(dtModel)

end = timestamp.today()

training_time=end-start

print(training_time)

# Use test set here so we can measure the accuracy of our model on new data
dtpredictions = dtModel.transform(test)


# Evaluate best model
print('Accuracy:', evaluator.evaluate(dtpredictions))
bestModel = dtModel.bestModel


print (bestModel)

"""#Gradient boosting with hyperparameter tuning"""

gbt = GBTClassifier(featuresCol="features_scaled", labelCol = 'Revenue',maxIter=10)

gbparamGrid = (ParamGridBuilder()
            .addGrid(gbt.maxDepth, [2, 5, 10])
             .addGrid(gbt.maxBins, [10, 20, 40])
             .addGrid(gbt.maxIter, [5, 10, 20])
             .build())
gbevaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
# Create 5-fold CrossValidator
gbcv = CrossValidator(estimator = gbt,
                      estimatorParamMaps = gbparamGrid,
                      evaluator = gbevaluator ,
                      numFolds = 10)



# getting the current date and time
start = timestamp.today()

gbcvModel = gbcv.fit(train)

end = timestamp.today()

training_time=end-start

print(training_time)

gbpredictions = gbcvModel.transform(test)
print('Accuracy:', gbevaluator.evaluate(gbpredictions))
bestModel = gbcvModel.bestModel

# Best  Hyperparameters
print('numTrees - ', bestModel.getNumTrees)
print('maxDepth - ', bestModel.getOrDefault('maxDepth'))

"""#Hyper parameter tuning for Random forest"""

rf = RandomForestClassifier(featuresCol='features_scaled', labelCol = 'Revenue')
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \
    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \
    .build()
rfevaluator = BinaryClassificationEvaluator(labelCol="Revenue", rawPredictionCol="prediction")
rfhypertuning = CrossValidator(estimator=rf,
                          estimatorParamMaps=paramGrid,
                          evaluator=rfevaluator,
                          numFolds=10)



# getting the current date and time
start = timestamp.today()

rfhypertuning_model = rfhypertuning.fit(train)

end = timestamp.today()

training_time=end-start

print(training_time)


rfpredictions = rfhypertuning_model.transform(test)
print('Accuracy:', rfevaluator.evaluate(rfpredictions))
bestModel = rfhypertuning_model.bestModel

# Best  Hyperparameters
print('numTrees - ', bestModel.getNumTrees)
print('maxDepth - ', bestModel.getOrDefault('maxDepth'))